{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hybrid Modle_alternative Keras for MDN-RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anguslou/Hybrid_Intelligence_System/blob/master/Hybrid_Modle_alternative_Keras_for_MDN_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "EqE235VcF8R3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Building the MDN-RNN model\n",
        " \n",
        "# Importing the libraries\n",
        "import math\n",
        "import numpy as np\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        " \n",
        "# Setting the dimensions of the latent vectors\n",
        "Z_DIM = 32\n",
        " \n",
        "# Setting the number of actions\n",
        "ACTION_DIM = 3\n",
        " \n",
        "# Setting the number of LSTM units\n",
        "HIDDEN_UNITS = 256\n",
        " \n",
        "# Setting the number of gaussian mixture outputs\n",
        "GAUSSIAN_MIXTURES = 5\n",
        " \n",
        "# Setting the batch size and number of epochs\n",
        "BATCH_SIZE =32\n",
        "EPOCHS = 20\n",
        " \n",
        "# Getting the gaussian mixture coeficients\n",
        "def get_mixture_coef(y_pred):\n",
        "    d = GAUSSIAN_MIXTURES * Z_DIM\n",
        "    rollout_length = K.shape(y_pred)[1]\n",
        "    pi = y_pred[:,:,:d]\n",
        "    mu = y_pred[:,:,d:(2*d)]\n",
        "    log_sigma = y_pred[:,:,(2*d):(3*d)]\n",
        "    pi = K.reshape(pi, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
        "    mu = K.reshape(mu, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
        "    log_sigma = K.reshape(log_sigma, [-1, rollout_length, GAUSSIAN_MIXTURES, Z_DIM])\n",
        "    pi = K.exp(pi) / K.sum(K.exp(pi), axis=2, keepdims=True)\n",
        "    sigma = K.exp(log_sigma)\n",
        "    return pi, mu, sigma\n",
        " \n",
        "# Normalizing the target values\n",
        "def tf_normal(y_true, mu, sigma, pi):\n",
        "    rollout_length = K.shape(y_true)[1]\n",
        "    y_true = K.tile(y_true,(1,1,GAUSSIAN_MIXTURES))\n",
        "    y_true = K.reshape(y_true, [-1, rollout_length, GAUSSIAN_MIXTURES,Z_DIM])\n",
        "    oneDivSqrtTwoPI = 1 / math.sqrt(2*math.pi)\n",
        "    result = y_true - mu\n",
        "    result = result * (1 / (sigma + 1e-8))\n",
        "    result = -K.square(result)/2\n",
        "    result = K.exp(result) * (1/(sigma + 1e-8))*oneDivSqrtTwoPI\n",
        "    result = result * pi\n",
        "    result = K.sum(result, axis=2)\n",
        "    return result\n",
        " \n",
        "# Building the MDN-RNN model within a class\n",
        " \n",
        "class MDNRNN():\n",
        " \n",
        "    # Initializing all the parameters and variables of the MDNRNN class\n",
        "    def __init__(self):\n",
        "        self.models = self._build()\n",
        "        self.model = self.models[0]\n",
        "        self.forward = self.models[1]\n",
        "        self.z_dim = Z_DIM\n",
        "        self.action_dim = ACTION_DIM\n",
        "        self.hidden_units = HIDDEN_UNITS\n",
        "        self.gaussian_mixtures = GAUSSIAN_MIXTURES\n",
        " \n",
        "    # Building the model\n",
        "    def _build(self):\n",
        "        # Defining the Inputs of the RNN (latent vector space + action space)\n",
        "        rnn_x = Input(shape=(None, Z_DIM + ACTION_DIM))\n",
        "        # Defining the LSTM layer that returns the output weights and cell states\n",
        "        lstm = LSTM(HIDDEN_UNITS, return_sequences=True, return_state = True)\n",
        "        # Getting the real outputs from the LSTM\n",
        "        lstm_output, _ , _ = lstm(rnn_x)\n",
        "        # Getting the gaussian mixture outputs\n",
        "        mdn = Dense(GAUSSIAN_MIXTURES * (3*Z_DIM))(lstm_output)\n",
        "        # Getting the training model\n",
        "        rnn = Model(rnn_x, mdn)\n",
        "        # Getting the hidden state and cell state inputs\n",
        "        state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
        "        state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
        "        # Grouping them\n",
        "        state_inputs = [state_input_h, state_input_c]\n",
        "        # Getting outputs new state and new cell state from the LSTM\n",
        "        _ , state_h, state_c = lstm(rnn_x, initial_state = [state_input_h, state_input_c])\n",
        "        # Defining the forward propagation for inference only\n",
        "        forward = Model([rnn_x] + state_inputs, [state_h, state_c])\n",
        "        # Implementing the training operations\n",
        "        def rnn_r_loss(y_true, y_pred):\n",
        "            # Defining the negative log loss over all the gausian mixtures\n",
        "            pi, mu, sigma = get_mixture_coef(y_pred)\n",
        "            result = tf_normal(y_true, mu, sigma, pi)\n",
        "            result = -K.log(result + 1e-8)\n",
        "            result = K.mean(result, axis = (1,2))\n",
        "            return result\n",
        "        # Defining the KL divergence loss, the same as in the VAE, only over normalized outputs\n",
        "        def rnn_kl_loss(y_true, y_pred):\n",
        "            pi, mu, sigma = get_mixture_coef(y_pred)\n",
        "            kl_loss = - 0.5 * K.mean(1 + K.log(K.square(sigma)) - K.square(mu) - K.square(sigma), axis = [1,2,3])\n",
        "            return kl_loss\n",
        "        # Defining the RNN loss\n",
        "        def rnn_loss(y_true, y_pred):\n",
        "            return rnn_r_loss(y_true, y_pred)\n",
        "        # Compiling the RNN model with the RNN loss and the RMSProp optimizer\n",
        "        rnn.compile(loss=rnn_loss, optimizer='rmsprop', metrics = [rnn_r_loss, rnn_kl_loss])\n",
        "        return (rnn,forward)\n",
        " \n",
        "    # Loading the weights of the model\n",
        "    def set_weights(self, filepath):\n",
        "        self.model.load_weights(filepath)\n",
        " \n",
        "    # Creating early stopping callbacks to prevent overfitting\n",
        "    def train(self, rnn_input, rnn_output, validation_split = 0.2):\n",
        "        earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=5, verbose=1, mode='auto')\n",
        "        callbacks_list = [earlystop]\n",
        "        # Fitting the model to the RNN inputs and targets\n",
        "        self.model.fit(rnn_input, rnn_output,\n",
        "            shuffle=True,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            validation_split=validation_split,\n",
        "            callbacks=callbacks_list)\n",
        "        # Saving the model after the training is done\n",
        "        self.model.save_weights('rnn/weights.h5')\n",
        " \n",
        "    # Separating the function used to save the model (usefull if the model is retrained)\n",
        "    def save_weights(self, filepath):\n",
        "        self.model.save_weights(filepath)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}